{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER using TRAX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvjb/Y9sfQ/Cp5Q6v2ode3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suhit97/Trax-Name-Entity-Recognition/blob/main/NER_using_TRAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJhDpKgzOF3u",
        "outputId": "ea90abe6-83a7-4400-8f94-bc5eea946199"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0bvz6QYUwXe",
        "outputId": "751e5da2-839c-4e4d-8fb7-fa98ac67c0b5"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/NER"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/NER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN0KFyyWeqXe"
      },
      "source": [
        "!pip -q install trax==1.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBwEt2jxOKsT"
      },
      "source": [
        "\r\n",
        "\r\n",
        "import trax \r\n",
        "from trax import layers as tl\r\n",
        "import os \r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "import random as rnd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyUkf5ZjQF8G"
      },
      "source": [
        "def get_vocab(vocab_path, tags_path):\r\n",
        "    vocab = {}\r\n",
        "    with open(vocab_path) as f:\r\n",
        "        for i, l in enumerate(f.read().splitlines()):\r\n",
        "            vocab[l] = i  # to avoid the 0\r\n",
        "        # loading tags (we require this to map tags to their indices)\r\n",
        "    vocab['<PAD>'] = len(vocab) # 35180\r\n",
        "    tag_map = {}\r\n",
        "    with open(tags_path) as f:\r\n",
        "        for i, t in enumerate(f.read().splitlines()):\r\n",
        "            tag_map[t] = i \r\n",
        "    \r\n",
        "    return vocab, tag_map\r\n",
        "\r\n",
        "def get_params(vocab, tag_map, sentences_file, labels_file):\r\n",
        "    sentences = []\r\n",
        "    labels = []\r\n",
        "\r\n",
        "    with open(sentences_file) as f:\r\n",
        "        for sentence in f.read().splitlines():\r\n",
        "            # replace each token by its index if it is in vocab\r\n",
        "            # else use index of UNK_WORD\r\n",
        "            s = [vocab[token] if token in vocab \r\n",
        "                 else vocab['UNK']\r\n",
        "                 for token in sentence.split(' ')]\r\n",
        "            sentences.append(s)\r\n",
        "\r\n",
        "    with open(labels_file) as f:\r\n",
        "        for sentence in f.read().splitlines():\r\n",
        "            # replace each label by its index\r\n",
        "            l = [tag_map[label] for label in sentence.split(' ')] # I added plus 1 here\r\n",
        "            labels.append(l) \r\n",
        "    return sentences, labels, len(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shxc-XFZRBE2",
        "outputId": "13b16179-e4c7-454f-aac6-b025238dbbc4"
      },
      "source": [
        "data = pd.read_csv(\"ner_dataset.csv\", encoding = \"ISO-8859-1\") \r\n",
        "train_sents = open('data/small/train/sentences.txt', 'r').readline()\r\n",
        "train_labels = open('data/small/train/labels.txt', 'r').readline()\r\n",
        "print('SENTENCE:', train_sents)\r\n",
        "print('SENTENCE LABEL:', train_labels)\r\n",
        "print('ORIGINAL DATA:\\n', data.head(5))\r\n",
        "del(data, train_sents, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "\n",
            "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
            "\n",
            "ORIGINAL DATA:\n",
            "     Sentence #           Word  POS Tag\n",
            "0  Sentence: 1      Thousands  NNS   O\n",
            "1          NaN             of   IN   O\n",
            "2          NaN  demonstrators  NNS   O\n",
            "3          NaN           have  VBP   O\n",
            "4          NaN        marched  VBN   O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzuw0liyUBY6"
      },
      "source": [
        "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\r\n",
        "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\r\n",
        "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\r\n",
        "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM-DcaMOUh-h",
        "outputId": "766e58ba-1da6-42a9-eb82-8682a0e897ae"
      },
      "source": [
        "print('vocab[\"the\"]:', vocab[\"the\"])\r\n",
        "# Pad token\r\n",
        "print('padded token:', vocab['<PAD>'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab[\"the\"]: 9\n",
            "padded token: 35180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S_rY0p3fKuY",
        "outputId": "9bde5b27-b9af-4ade-b2f2-0ece25096c6e"
      },
      "source": [
        "print(tag_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utOoKZy1gF0G",
        "outputId": "5a9af843-573d-43a8-9642-1bd092b75c50"
      },
      "source": [
        "print('The number of outputs is tag_map', len(tag_map))\r\n",
        "# The number of vocabulary tokens (including <PAD>)\r\n",
        "g_vocab_size = len(vocab)\r\n",
        "print(f\"Num of vocabulary words: {g_vocab_size}\")\r\n",
        "print('The vocab size is', len(vocab))\r\n",
        "print('The training size is', t_size)\r\n",
        "print('The validation size is', v_size)\r\n",
        "print('An example of the first sentence is', t_sentences[0])\r\n",
        "print('An example of its corresponding label is', t_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of outputs is tag_map 17\n",
            "Num of vocabulary words: 35181\n",
            "The vocab size is 35181\n",
            "The training size is 33570\n",
            "The validation size is 7194\n",
            "An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
            "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnTH8nyXgSGd"
      },
      "source": [
        "## Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuXvv2nmgPny"
      },
      "source": [
        "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\r\n",
        "    \r\n",
        "    num_lines = len(x)\r\n",
        "    \r\n",
        "    # create an array with the indexes of data_lines that can be shuffled\r\n",
        "    lines_index = [*range(num_lines)]\r\n",
        "    \r\n",
        "    # shuffle the indexes if shuffle is set to True\r\n",
        "    if shuffle:\r\n",
        "        rnd.shuffle(lines_index)\r\n",
        "    \r\n",
        "    index = 0 # tracks current location in x, y\r\n",
        "    while True:\r\n",
        "        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\r\n",
        "        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\r\n",
        "                \r\n",
        " \r\n",
        "        max_len = 0\r\n",
        "        for i in range(batch_size):\r\n",
        "             # if the index is greater than or equal to the number of lines in x\r\n",
        "            if index >= num_lines:\r\n",
        "                # then reset the index to 0\r\n",
        "                index = 0\r\n",
        "                # re-shuffle the indexes if shuffle is set to True\r\n",
        "                if shuffle:\r\n",
        "                    rnd.shuffle(lines_index)\r\n",
        "            \r\n",
        "            # The current position is obtained using `lines_index[index]`\r\n",
        "            # Store the x value at the current position into the buffer_x\r\n",
        "            buffer_x[i] = x[lines_index[index]]\r\n",
        "            \r\n",
        "            # Store the y value at the current position into the buffer_y\r\n",
        "            buffer_y[i] = y[lines_index[index]]\r\n",
        "            \r\n",
        "            lenx = len(x[lines_index[index]])    #length of current x[]\r\n",
        "            if lenx > max_len:\r\n",
        "                max_len = lenx                   #max_len tracks longest x[]\r\n",
        "            \r\n",
        "            # increment index by one\r\n",
        "            index += 1\r\n",
        "\r\n",
        "\r\n",
        "        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\r\n",
        "        X = np.full((batch_size, max_len), pad)\r\n",
        "        Y = np.full((batch_size, max_len), pad)\r\n",
        "\r\n",
        "        # copy values from lists to NumPy arrays. Use the buffered values\r\n",
        "        for i in range(batch_size):\r\n",
        "            # get the example (sentence as a tensor)\r\n",
        "            # in `buffer_x` at the `i` index\r\n",
        "            x_i = buffer_x[i]\r\n",
        "            \r\n",
        "            # similarly, get the example's labels\r\n",
        "            # in `buffer_y` at the `i` index\r\n",
        "            y_i = buffer_y[i]\r\n",
        "            \r\n",
        "            # Walk through each word in x_i\r\n",
        "            for j in range(len(x_i)):\r\n",
        "                # store the word in x_i at position j into X\r\n",
        "                X[i, j] = x_i[j]\r\n",
        "                \r\n",
        "                # store the label in y_i at position j into Y\r\n",
        "                Y[i, j] = y_i[j]\r\n",
        "\r\n",
        "        if verbose: print(\"index=\", index)\r\n",
        "        yield((X,Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV3C3kO7hWqd",
        "outputId": "17f0504a-0c68-4f31-f1c3-d4b89a421789"
      },
      "source": [
        "batch_size = 5\r\n",
        "mini_sentences = t_sentences[0: 8]\r\n",
        "mini_labels = t_labels[0: 8]\r\n",
        "dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[\"<PAD>\"], shuffle=False, verbose=True)\r\n",
        "X1, Y1 = next(dg)\r\n",
        "X2, Y2 = next(dg)\r\n",
        "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\r\n",
        "print(X1[0][:], \"\\n\", Y1[0][:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index= 5\n",
            "index= 2\n",
            "(5, 30) (5, 30) (5, 30) (5, 30)\n",
            "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
            "    12    13    14     9    15     1    16    17    18    19    20    21\n",
            " 35180 35180 35180 35180 35180 35180] \n",
            " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
            "     1     0     0     0     0     0     2     0     0     0     0     0\n",
            " 35180 35180 35180 35180 35180 35180]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MFifIuhh1D9"
      },
      "source": [
        "def NER(vocab_size=35181, d_model=50, tags=tag_map):\r\n",
        "    \r\n",
        "    model = tl.Serial(\r\n",
        "      tl.Embedding(vocab_size, d_model), \r\n",
        "      tl.LSTM(d_model), \r\n",
        "      tl.Dense(len(tags)), \r\n",
        "      tl.LogSoftmax()  \r\n",
        "      )\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgJ64i7zibGg",
        "outputId": "cf529474-bb54-48d6-957b-b02ad15cb389"
      },
      "source": [
        "model = NER()\r\n",
        "\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Embedding_35181_50\n",
            "  LSTM_50\n",
            "  Dense_17\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-gGVmQvkzSo"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "# Create training data\r\n",
        "train_generator = trax.supervised.inputs.add_loss_weights(\r\n",
        "    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], True),\r\n",
        "    id_to_mask=vocab['<PAD>'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Create validation data\r\n",
        "eval_generator = trax.supervised.inputs.add_loss_weights(\r\n",
        "    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], True),\r\n",
        "    id_to_mask=vocab['<PAD>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMpiM4RzkDPC"
      },
      "source": [
        "def train_model(NER, train_generator, eval_generator, train_steps, output_dir='model'):\r\n",
        "    \r\n",
        "    train_task = training.TrainTask(\r\n",
        "        train_generator,\r\n",
        "        loss_layer = tl.CrossEntropyLoss(),\r\n",
        "         optimizer = trax.optimizers.Adam(0.01) )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask(\r\n",
        "      labeled_data = eval_generator, # A labeled data generator\r\n",
        "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\r\n",
        "      n_eval_batches = 10  # Number of batches to use on each evaluation\r\n",
        "    )\r\n",
        "    \r\n",
        "    training_loop = training.Loop(\r\n",
        "        NER, # A model to train\r\n",
        "        train_task, # A train task\r\n",
        "        eval_task = eval_task, # The evaluation task\r\n",
        "        output_dir = output_dir) \r\n",
        "    \r\n",
        "    training_loop.run(n_steps = train_steps)\r\n",
        "\r\n",
        "    return training_loop\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlZsc2P7m1Dt",
        "outputId": "1a4fbf18-af60-4b57-d54d-4924f75aad63"
      },
      "source": [
        "train_steps = 100 \r\n",
        "!rm -f 'model/model.pkl.gz'\r\n",
        "training_loop = train_model(NER(), train_generator, eval_generator, train_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step      1: train CrossEntropyLoss |  3.11000991\n",
            "Step      1: eval  CrossEntropyLoss |  2.03229570\n",
            "Step      1: eval          Accuracy |  0.59135034\n",
            "Step    100: train CrossEntropyLoss |  0.59521270\n",
            "Step    100: eval  CrossEntropyLoss |  0.35065396\n",
            "Step    100: eval          Accuracy |  0.91579654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTw4Z9MftivD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4s3KWhOm9AR"
      },
      "source": [
        "model = NER()\r\n",
        "model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\r\n",
        "\r\n",
        "# Load the pretrained model\r\n",
        "model.init_from_file('model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmZW5BlJtfIG"
      },
      "source": [
        "def evaluate_prediction(pred, labels, pad):\r\n",
        "    \r\n",
        "    outputs = np.argmax(pred, axis=2)\r\n",
        "    print(\"outputs shape:\", outputs.shape)\r\n",
        "\r\n",
        "    mask = labels != pad\r\n",
        "    print(\"mask shape:\", mask.shape, \"mask[0][20:30]:\", mask[0][20:30])\r\n",
        "    \r\n",
        "    accuracy = np.sum(outputs == labels) / float(np.sum(mask))\r\n",
        "    \r\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV6G3OZH0hAd",
        "outputId": "9b416996-4ea2-41bc-81e8-da81d806ea94"
      },
      "source": [
        "x, y = next(data_generator(len(test_sentences), test_sentences, test_labels, vocab['<PAD>']))\r\n",
        "print(\"input shapes\", x.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shapes (7194, 70) (7194, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dhVSp090via",
        "outputId": "5a9af96a-4e13-4a90-b09c-c38efc631453"
      },
      "source": [
        "tmp_pred = model(x)\r\n",
        "print(type(tmp_pred))\r\n",
        "print(f\"tmp_pred has shape: {tmp_pred.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'jax.interpreters.xla._DeviceArray'>\n",
            "tmp_pred has shape: (7194, 70, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyHjn8ae0wV5",
        "outputId": "38155cdf-5c3f-40a3-ad6d-ff9dbbefec19"
      },
      "source": [
        "accuracy = evaluate_prediction(model(x), y, vocab['<PAD>'])\r\n",
        "print(\"accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "outputs shape: (7194, 70)\n",
            "mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]\n",
            "accuracy:  0.91168374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmIhNgUr00Zq"
      },
      "source": [
        "def predict(sentence, model, vocab, tag_map):\r\n",
        "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\r\n",
        "    batch_data = np.ones((1, len(s)))\r\n",
        "    batch_data[0][:] = s\r\n",
        "    sentence = np.array(batch_data).astype(int)\r\n",
        "    output = model(sentence)\r\n",
        "    outputs = np.argmax(output, axis=2)\r\n",
        "    labels = list(tag_map.keys())\r\n",
        "    pred = []\r\n",
        "    for i in range(len(outputs[0])):\r\n",
        "        idx = outputs[0][i] \r\n",
        "        pred_label = labels[idx]\r\n",
        "        pred.append(pred_label)\r\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvbItVN209-L",
        "outputId": "a2394145-6e78-4738-9e2b-d11005f8afe2"
      },
      "source": [
        "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldnâ€™t necessarily come\"\r\n",
        "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\r\n",
        "predictions = predict(sentence, model, vocab, tag_map)\r\n",
        "for x,y in zip(sentence.split(' '), predictions):\r\n",
        "    if y != 'O':\r\n",
        "        print(x,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "White B-org\n",
            "House I-org\n",
            "U.S, B-geo\n",
            "Sunday B-tim\n",
            "morning I-org\n",
            "White B-org\n",
            "House I-org\n",
            "coronavirus B-geo\n",
            "fall, B-geo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iKw26JH0-bV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}